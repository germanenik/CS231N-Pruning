gerashabanets@cp-vton-plus-vm-vm:~/CS231N-Pruning$ ls
LICENSE      architectures     data      networks.py           result       test.py      visualization.py
Pipfile      checkpoints       exp5.txt  networks_baseline.py  ssim.py      torchpruner  visualize_weights.py
README.mcriteria = (criterionL1, gicloss)pruning.py            teaser.png   train.py     weights_visualizations
__pycachattribution = APoZAttributionMetric(model, train_loader.data_loader, criteria, device=device)
gerashabpruner = Pruner(model, input_size=get_GMM_input_size(train_loader), device=device, optimizer=finetuning_optimizer)
Vim: Warning: Output is not to a terminal
    if osubmodels = [model.extractionA.model, model.extractionB.model, model.regression.conv]
        for submodel in submodels:
            layers_of_interest = [layer for layer in submodel.children() if isinstance(layer, torch.nn.modules.conv._ConvNd) or is
instance(layer, nn.BatchNorm2d)]
            num_conv = len([1 for layer in  layers_of_interest if isinstance(layer, torch.nn.modules.conv._ConvNd)])
            for idx, module in enumerate(layers_of_interest):
                if not isinstance(module, nn.Conv2d):
                    continue
                num_conv -= 1

                if num_conv == 0:
                    break #do not prune the last one bc messes up dims

                print("interest layer num:", idx)
                # Compute Weight Value attributions
                attr = attribution.run(module)
                pruning_indices = [idx for idx, val in enumerate(attr) if val == 0]
                breakpoint()
                # # weightnorm
                # k = int(len(attr) / 20) #5%
                # pruning_indices = np.argpartition(attr, k)[:k]

                cascading = layers_of_interest[idx+1:]
                print("cascading layers", cascading)
                pruner.prune_model(module, indices=pruning_indices, cascading_modules=cascading)
                # train for a few epochs

                pretty_print_dims(get_pruned_dimensions(submodel))
                _train_gmm(opt, train_loader, model, criterionL1, gicloss, finetuning_optimizer, board, opt.finetune_steps_brief)
#14600 / 4 * 2 = 7000


        #carefully finetune prunced model
        pretty_print_dims(get_pruned_dimensions(submodel))
        _train_gmm(opt, train_loader, model, criterionL1, gicloss, finetuning_optimizer, board, opt.finetune_steps_careful) #35000
        torch.save(model, "architectures/pruned_GMM")

[1]+  Stopped                 vim train.py | grep attr
gerashabanets@cp-vton-plus-vm-vm:~/CS231N-Pruning$
gerashabanets@cp-vton-plus-vm-vm:~/CS231N-Pruning$
gerashabanets@cp-vton-plus-vm-vm:~/CS231N-Pruning$
gerashabanets@cp-vton-plus-vm-vm:~/CS231N-Pruning$
gerashabanets@cp-vton-plus-vm-vm:~/CS231N-Pruning$
gerashabanets@cp-vton-plus-vm-vm:~/CS231N-Pruning$
gerashabanets@cp-vton-plus-vm-vm:~/CS231N-Pruning$
gerashabanets@cp-vton-plus-vm-vm:~/CS231N-Pruning$
gerashabanets@cp-vton-plus-vm-vm:~/CS231N-Pruning$
gerashabanets@cp-vton-plus-vm-vm:~/CS231N-Pruning$
gerashabanets@cp-vton-plus-vm-vm:~/CS231N-Pruning$
gerashabanets@cp-vton-plus-vm-vm:~/CS231N-Pruning$ clear
gerashabanets@cp-vton-plus-vm-vm:~/CS231N-Pruning$
gerashabanets@cp-vton-plus-vm-vm:~/CS231N-Pruning$
gerashabanets@cp-vton-plus-vm-vm:~/CS231N-Pruning$
gerashabanets@cp-vton-plus-vm-vm:~/CS231N-Pruning$ python train.py --name GMM --stage GMM --debug --finetune_steps_brief 0 --finet
une_steps_careful 0
Namespace(batch_size=4, checkpoint='', checkpoint_dir='checkpoints', data_list='train_pairs.txt', datamode='train', dataroot='data
', debug=True, decay_step=100000, display_count=20, fine_height=256, fine_width=192, finetune_steps_brief=0, finetune_steps_carefu
l=0, gpu_ids='', grid_size=5, keep_step=100000, lr=0.0001, name='GMM', radius=5, save_count=5000, shuffle=False, stage='GMM', tens
orboard_dir='tensorboard', workers=1)
Start to train stage: GMM, named: GMM!
initialization method [normal]
initialization method [normal]
interest layer num: 0
0
Traceback (most recent call last):
  File "train.py", line 332, in <module>
    main()
  File "train.py", line 313, in main
    train_gmm(opt, train_loader, model, board)
  File "train.py", line 111, in train_gmm
    attr = attribution.run(module)
  File "/home/gerashabanets/CS231N-Pruning/torchpruner/torchpruner/attributions/methods/apoz.py", line 19, in run
    self.run_all_forward()
  File "/home/gerashabanets/CS231N-Pruning/torchpruner/torchpruner/attributions/attributions.py", line 66, in run_all_forward
    grid, theta = self.model(agnostic, cm)    # can be added c too for new training
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/gerashabanets/CS231N-Pruning/networks.py", line 520, in forward
    featureA = self.extractionA(inputA)
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/gerashabanets/CS231N-Pruning/networks.py", line 80, in forward
    return self.model(x)
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/container.py", line 119, in forward
    input = module(input)
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py", line 399, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py", line 396, in _conv_forward
    self.padding, self.dilation, self.groups)
RuntimeError: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same
gerashabanets@cp-vton-plus-vm-vm:~/CS231N-Pruning$ python train.py --name GMM --stage GMM --debug --finetune_steps_brief 0 --finet
une_steps_careful 0
Namespace(batch_size=4, checkpoint='', checkpoint_dir='checkpoints', data_list='train_pairs.txt', datamode='train', dataroot='data
', debug=True, decay_step=100000, display_count=20, fine_height=256, fine_width=192, finetune_steps_brief=0, finetune_steps_carefu
l=0, gpu_ids='', grid_size=5, keep_step=100000, lr=0.0001, name='GMM', radius=5, save_count=5000, shuffle=False, stage='GMM', tens
orboard_dir='tensorboard', workers=1)
Start to train stage: GMM, named: GMM!
initialization method [normal]
initialization method [normal]
interest layer num: 0
0
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:3826: UserWarning: Default grid_sample and affine_grid behavior has
changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentatio
n of grid_sample for details.
  "Default grid_sample and affine_grid behavior has changed "
1
2
> /home/gerashabanets/CS231N-Pruning/train.py(118)train_gmm()
-> cascading = layers_of_interest[idx+1:]
(Pdb) q
Traceback (most recent call last):
  File "train.py", line 332, in <module>
    main()
  File "train.py", line 313, in main
    train_gmm(opt, train_loader, model, board)
  File "train.py", line 118, in train_gmm
    cascading = layers_of_interest[idx+1:]
  File "train.py", line 118, in train_gmm
    cascading = layers_of_interest[idx+1:]
  File "/opt/conda/lib/python3.7/bdb.py", line 88, in trace_dispatch
    return self.dispatch_line(frame)
  File "/opt/conda/lib/python3.7/bdb.py", line 113, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit
gerashabanets@cp-vton-plus-vm-vm:~/CS231N-Pruning$ python train.py --name GMM --stage GMM --debug --finetune_steps_brief 0 --finet
une_steps_careful 0
Namespace(batch_size=4, checkpoint='', checkpoint_dir='checkpoints', data_list='train_pairs.txt', datamode='train', dataroot='data
', debug=True, decay_step=100000, display_count=20, fine_height=256, fine_width=192, finetune_steps_brief=0, finetune_steps_carefu
l=0, gpu_ids='', grid_size=5, keep_step=100000, lr=0.0001, name='GMM', radius=5, save_count=5000, shuffle=False, stage='GMM', tens
orboard_dir='tensorboard', workers=1)
Start to train stage: GMM, named: GMM!
initialization method [normal]
initialization method [normal]
interest layer num: 0
0
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:3826: UserWarning: Default grid_sample and affine_grid behavior has
changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentatio
n of grid_sample for details.
  "Default grid_sample and affine_grid behavior has changed "
1
2
cascading layers [BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False), Conv2d(64, 128, kernel_size=(4
, 4), stride=(2, 2), padding=(1, 1)), BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False), Conv2d(12
8, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)), BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_s
tats=False), Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)), BatchNorm2d(512, eps=1e-05, momentum=0.1, affine
=True, track_running_stats=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(512, eps=1e-05
, momentum=0.1, affine=True, track_running_stats=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
Pruning 4 units from BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False) (in)
Pruning 4 units from Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)) (in)
Pruning 4 units from Conv2d(22, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)) (out)
(0): Conv2d torch.Size([60, 22, 4, 4])
(1): ReLU None
(2): BatchNorm2d torch.Size([60])
(3): Conv2d torch.Size([128, 60, 4, 4])
(4): ReLU None
(5): BatchNorm2d torch.Size([128])
(6): Conv2d torch.Size([256, 128, 4, 4])
(7): ReLU None
(8): BatchNorm2d torch.Size([256])
(9): Conv2d torch.Size([512, 256, 4, 4])
(10): ReLU None
(11): BatchNorm2d torch.Size([512])
(12): Conv2d torch.Size([512, 512, 3, 3])
(13): ReLU None
(14): BatchNorm2d torch.Size([512])
(15): Conv2d torch.Size([512, 512, 3, 3])
(16): ReLU None
interest layer num: 2
0
1
2
cascading layers [BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False), Conv2d(128, 256, kernel_size=
(4, 4), stride=(2, 2), padding=(1, 1)), BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False), Conv2d(
256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)), BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running
_stats=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(512, eps=1e-05, momentum=0.1, affi
ne=True, track_running_stats=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
Traceback (most recent call last):
  File "train.py", line 331, in <module>
    main()
  File "train.py", line 312, in main
    train_gmm(opt, train_loader, model, board)
  File "train.py", line 119, in train_gmm
    pruner.prune_model(module, indices=pruning_indices, cascading_modules=cascading)
  File "/home/gerashabanets/CS231N-Pruning/torchpruner/torchpruner/pruner/pruner.py", line 40, in prune_model
    self._run_forward()
  File "/home/gerashabanets/CS231N-Pruning/torchpruner/torchpruner/pruner/pruner.py", line 187, in _run_forward
    y = self.model(*x)
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/gerashabanets/CS231N-Pruning/networks.py", line 520, in forward
    featureA = self.extractionA(inputA)
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/gerashabanets/CS231N-Pruning/networks.py", line 80, in forward
    return self.model(x)
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/container.py", line 119, in forward
    input = module(input)
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 893, in _call_impl
    hook_result = hook(self, input, result)
  File "/home/gerashabanets/CS231N-Pruning/torchpruner/torchpruner/pruner/pruner.py", line 140, in _hook
    torch.tensor(np.nan).to(self.device),
IndexError: index_fill_(): Expected dtype int64 for index.
gerashabanets@cp-vton-plus-vm-vm:~/CS231N-Pruning$ python train.py --name GMM --stage GMM --debug --finetune_steps_brief 0 --finet
une_steps_careful 0
Namespace(batch_size=4, checkpoint='', checkpoint_dir='checkpoints', data_list='train_pairs.txt', datamode='train', dataroot='data
', debug=True, decay_step=100000, display_count=20, fine_height=256, fine_width=192, finetune_steps_brief=0, finetune_steps_carefu
l=0, gpu_ids='', grid_size=5, keep_step=100000, lr=0.0001, name='GMM', radius=5, save_count=5000, shuffle=False, stage='GMM', tens
orboard_dir='tensorboard', workers=1)
Start to train stage: GMM, named: GMM!
initialization method [normal]
initialization method [normal]
interest layer num: 0
0
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:3826: UserWarning: Default grid_sample and affine_grid behavior has
changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentatio
n of grid_sample for details.
  "Default grid_sample and affine_grid behavior has changed "
1
2
[3.6212500e+02 1.2021000e+04 2.2975000e+02 5.6778750e+03 3.1500000e+01
 0.0000000e+00 1.2264375e+04 1.2268375e+04 1.2074000e+04 1.1250000e+00
 1.1847250e+04 1.8625000e+01 2.0000000e+00 1.2104375e+04 1.2249125e+04
 5.5900000e+02 2.7500000e+00 7.3641250e+03 5.8278750e+03 2.8625000e+01
 8.9487500e+02 0.0000000e+00 1.2066250e+04 4.8225000e+02 4.9632500e+03
 1.2285750e+04 2.9075000e+02 2.3250000e+01 4.0887500e+02 2.3426250e+03
 1.1891125e+04 5.4670000e+03 4.0987500e+02 1.2251625e+04 1.2164875e+04
 1.2287125e+04 5.1437500e+02 0.0000000e+00 2.3750000e+00 2.5962500e+02
 1.6875000e+02 1.2181500e+04 5.7375000e+01 1.2000000e+01 5.2287500e+03
 1.2164625e+04 1.5750000e+01 6.0051250e+03 7.8753750e+03 1.2288000e+04
 6.5281250e+03 2.2875000e+02 1.2188625e+04 2.2825000e+02 1.2287625e+04
 6.4875000e+01 1.1647000e+04 2.5287500e+02 1.9900000e+02 1.2025000e+02
 1.0000000e+00 1.0000000e+00 0.0000000e+00 4.7225000e+02]
indices to prune: 4
cascading layers [BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False), Conv2d(64, 128, kernel_size=(4
, 4), stride=(2, 2), padding=(1, 1)), BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False), Conv2d(12
8, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)), BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_s
tats=False), Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)), BatchNorm2d(512, eps=1e-05, momentum=0.1, affine
=True, track_running_stats=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(512, eps=1e-05
, momentum=0.1, affine=True, track_running_stats=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
/home/gerashabanets/CS231N-Pruning/torchpruner/torchpruner/pruner/pruner.py:139: UserWarning: To copy construct from a tensor, it
is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tenso
r(sourceTensor).
  torch.tensor(indices).to(self.device),
Pruning 4 units from BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False) (in)
Pruning 4 units from Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)) (in)
Pruning 4 units from Conv2d(22, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)) (out)
(0): Conv2d torch.Size([60, 22, 4, 4])
(1): ReLU None
(2): BatchNorm2d torch.Size([60])
(3): Conv2d torch.Size([128, 60, 4, 4])
(4): ReLU None
(5): BatchNorm2d torch.Size([128])
(6): Conv2d torch.Size([256, 128, 4, 4])
(7): ReLU None
(8): BatchNorm2d torch.Size([256])
(9): Conv2d torch.Size([512, 256, 4, 4])
(10): ReLU None
(11): BatchNorm2d torch.Size([512])
(12): Conv2d torch.Size([512, 512, 3, 3])
(13): ReLU None
(14): BatchNorm2d torch.Size([512])
(15): Conv2d torch.Size([512, 512, 3, 3])
(16): ReLU None
interest layer num: 2
0
1
2
[1410.75  2042.75  1365.    1442.625 1527.    1307.875 1588.625 1338.75
 1735.375 1492.375 1616.625 1203.625 1554.25   393.    1775.875 1616.25
 1955.    1325.    1739.5   1864.375 1451.    1577.25  1545.625 1413.875
 1755.625 1513.125 1473.625  939.75  1872.     519.625 1641.375 1880.625
 1074.375 1541.625 1566.25  1402.75  1389.    1639.625 1575.375 1530.625
 1405.25  1835.875 1475.    1330.875 1230.25  1579.125 1426.75  1651.625
 1667.    1375.25  1794.5   1467.5   1514.625 1618.125 1802.625 1307.875
 1385.5   1488.375 1696.75  1552.375 1476.5   1818.25  2442.75  1537.875
 1715.75   483.75  1502.125 1047.625 1713.5    957.375 1078.     328.
 1338.375 1778.625 1952.875  385.375 1549.625 1684.375 1467.375 1508.625
 1870.875 1460.875 1641.125 1609.875 1651.25  2086.5   1679.125 1816.25
 2045.    1596.125 1452.625 1373.    2684.125 1465.875 1849.75  1426.25
 1410.75  1480.25  1569.875 1255.375 2251.875 2007.625 1374.375 1453.625
 1432.    1449.75  1421.625 1521.125 1650.    1898.125 1579.125 1220.
 1666.75  1675.625 1546.125 1321.25  1784.5   1435.625 1362.75  2713.375
 1505.375 1332.625 1223.5   1430.75  1305.25  1677.75  1167.375 1200.   ]
indices to prune: 0
cascading layers [BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False), Conv2d(128, 256, kernel_size=
(4, 4), stride=(2, 2), padding=(1, 1)), BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False), Conv2d(
256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)), BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running
_stats=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(512, eps=1e-05, momentum=0.1, affi
ne=True, track_running_stats=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
Pruning 0 units from Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)) (out)
(0): Conv2d torch.Size([60, 22, 4, 4])
(1): ReLU None
(2): BatchNorm2d torch.Size([60])
(3): Conv2d torch.Size([128, 60, 4, 4])
(4): ReLU None
(5): BatchNorm2d torch.Size([128])
(6): Conv2d torch.Size([256, 128, 4, 4])
(7): ReLU None
(8): BatchNorm2d torch.Size([256])
(9): Conv2d torch.Size([512, 256, 4, 4])
(10): ReLU None
(11): BatchNorm2d torch.Size([512])
(12): Conv2d torch.Size([512, 512, 3, 3])
(13): ReLU None
(14): BatchNorm2d torch.Size([512])
(15): Conv2d torch.Size([512, 512, 3, 3])
(16): ReLU None
interest layer num: 4
0
1
2
[296.25  330.875 376.25  320.625 334.875 439.125 480.125 375.    368.625
 291.625 399.5   386.    172.875 380.375 435.125 359.    471.5   426.5
 435.375 409.875 369.75  435.75  436.375 589.25  338.5   451.25  425.875
 431.875 395.75  521.875 348.5   449.25  499.5   401.    360.    519.125
 289.375 487.25  391.375 247.125 417.375 475.875 272.875 385.625 401.
 420.5   353.875 453.375 388.    378.5   306.5   264.125 430.25  320.25
 404.25  307.5   395.375 605.875 332.375 415.625 341.75  477.125 506.
 397.    346.5   307.5   240.75  169.25  259.    295.    360.75  347.
 334.75  406.375 525.75  377.375 395.    170.625 436.125 393.625 335.75
 371.375 417.75  368.75  364.875 313.25  371.875 425.    490.125 380.75
 422.375 439.125 410.625 435.25  297.5   351.875 457.375 412.25  516.75
 362.375 366.5   318.25  334.25  373.125 443.25  304.625 394.    515.
 387.25  523.375 339.125 410.25  341.375 351.375 309.25  325.625 343.75
 435.25  383.25  271.875 390.125 439.    384.125 505.75  277.375 545.625
 338.    501.    363.875 441.75  559.75  559.    290.    384.75  377.
 412.375 295.    372.75  348.5   324.625 426.75  372.    286.375 204.625
 331.5   380.375 269.25  545.25  386.75  324.125 430.    327.    547.375
 438.625 311.5   156.25  399.375 452.125 512.    448.75  271.75  367.
 193.75  395.875 379.5   366.25  463.5   451.25  361.25  331.875 466.375
 360.75  533.75  266.875 331.625 250.75  386.875 432.375 331.5   343.625
 428.5   327.5   358.5   454.125 436.75  369.75  491.625 408.25  512.
 437.625 359.875 465.625 498.875 333.625 303.875 286.25  431.75  451.375
 237.875 404.125 445.875 276.25  363.375 355.125 380.125 325.875 327.5
 364.125 349.75  314.375 413.    228.875 290.75  447.125 290.5   443.25
 470.875 482.25  349.625 448.875 381.75  446.375 437.    341.5   276.
 351.5   381.75  412.5   299.75  383.625 359.125 336.875 385.875 415.375
 398.375 372.25  299.    364.375 370.125 440.75  412.25  349.25  385.75
 432.625 391.375 459.625 362.375 457.25  463.375 359.375 398.625 281.25
 402.375 455.375 379.625 464.5  ]
indices to prune: 0
cascading layers [BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False), Conv2d(256, 512, kernel_size=
(4, 4), stride=(2, 2), padding=(1, 1)), BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False), Conv2d(
512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running
_stats=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
Pruning 0 units from Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)) (out)
(0): Conv2d torch.Size([60, 22, 4, 4])
(1): ReLU None
(2): BatchNorm2d torch.Size([60])
(3): Conv2d torch.Size([128, 60, 4, 4])
(4): ReLU None
(5): BatchNorm2d torch.Size([128])
(6): Conv2d torch.Size([256, 128, 4, 4])
(7): ReLU None
(8): BatchNorm2d torch.Size([256])
(9): Conv2d torch.Size([512, 256, 4, 4])
(10): ReLU None
(11): BatchNorm2d torch.Size([512])
(12): Conv2d torch.Size([512, 512, 3, 3])
(13): ReLU None
(14): BatchNorm2d torch.Size([512])
(15): Conv2d torch.Size([512, 512, 3, 3])
(16): ReLU None
interest layer num: 6
0
1
2
[ 80.     97.     87.875  75.75   91.875 106.    110.75   91.25   91.25
  70.25   93.375 102.5   124.625  91.     98.25  112.125  94.75   89.625
  85.25   94.125 114.125  96.5    75.625  98.     78.25  104.125  53.375
 103.375  86.5    84.875  86.75   65.875  93.625  93.25   94.125 111.125
 114.    111.125 102.25   94.375  72.875 103.     76.375  96.875 106.25
  87.625 102.125  86.875 101.875 116.    114.375  92.    102.25   86.75
  89.5    77.    104.125  66.     92.375  80.75   90.     78.5    98.875
  85.375 125.125 112.875  82.75   97.     92.75  134.875 105.     82.5
  97.875 101.25  104.125  99.375  92.125 100.5   101.625  81.875  99.
 100.875  95.125  82.375 102.875 101.25   98.375 123.875 100.75   90.375
  83.625 101.375  99.125  89.25   78.    101.375  90.    103.625  95.75
  98.25  105.     81.375 102.125  95.5   103.625 104.25   96.875 102.
 108.25   99.625  95.125 106.5    87.5    91.625 100.375 101.5    91.125
  94.5   115.75   81.75   96.125  87.75  105.125  90.375  70.75   95.875
  86.875  85.125 100.25   93.875  93.125 104.25  102.75   95.875  92.125
 107.375  68.75  108.125  95.375  94.375  91.625  71.25   85.5   104.625
  83.     81.25   90.75  102.875  91.75  111.25   94.75   91.75   90.375
  89.125  89.25  103.    110.25  111.75   87.875 125.375  96.875 107.125
 101.75  110.25  103.125  99.75  104.125  87.375  82.375  92.625  98.5
  96.375  81.75  110.25   92.875  82.5    97.     84.     72.625  94.375
  97.    100.625 113.     89.25   96.75   99.25   85.375  95.5    94.75
 109.    104.125  95.375 104.875  92.5    84.375  83.25   74.25  101.
  96.75   83.5   100.375  67.    102.    105.875 101.375 100.625  83.875
  89.625 103.25  107.875 111.125 101.125  95.375 104.375  88.5    72.5
 100.375  92.625 119.875  93.75   98.75   74.75   96.5    91.25   95.
 106.125 107.625 119.875 131.5    91.125 113.375  55.125  95.875  98.625
  89.875 101.     99.875 106.375  95.875 111.875  96.875  80.125  93.25
 113.125  80.75  110.5    98.     98.125 109.375  96.625  90.     77.875
  96.75  122.375  89.125  64.25   79.25  116.125  87.5    85.625 104.125
  99.375 109.125 123.75   98.875  88.75  114.125  87.875  73.     78.5
  84.75   93.    131.625 103.5    94.     92.125  87.    101.875 118.125
 119.875 100.875 100.625 121.625  93.5    69.875 105.125 105.25   90.875
 104.5   108.625 104.5    66.5    87.625  90.75  100.75   91.875  70.375
 103.625  92.625  89.875 105.375  84.25  104.125 130.375 100.625  89.25
  83.5   116.25   97.125  97.875  97.375  89.25  102.75  101.875  64.875
 100.625  96.875  76.5    81.    107.     94.625  89.5    89.875  81.125
  69.375  96.75  102.625 124.5   101.625 101.375  65.375  96.125  94.
  99.75  105.125  89.625 101.875  95.25   88.125 111.5    81.25   97.5
  77.    115.625  77.5    99.125  96.625  95.25   83.875  95.125 103.
  97.25   90.25   88.5    91.     75.5    96.25   99.25  101.875  96.75
  98.25   91.625  85.75  103.75   91.25   93.5   100.5    94.5    82.125
 108.25  100.5    96.875 109.25   99.25  114.25   88.    109.     78.5
  92.75  100.75   84.25   91.     70.25  107.5   124.5   130.25   95.75
  92.25  100.375  96.     99.    110.25   87.875  75.125  97.25   83.875
  84.75   97.375  91.     81.375  82.25   88.625 101.     88.     97.875
  82.25  106.375  84.625  91.75   95.75   89.75  102.     81.25   77.5
  77.     94.75   91.375 126.125  65.625  91.625  90.875 101.125  96.
  99.125  98.875 104.125  82.625 102.    114.75  110.75   72.125 103.25
  90.25   80.     85.    118.     88.375  96.5    95.25  100.625 106.125
  82.375  83.75   92.625  86.75  101.25   95.375  74.25   75.625 107.25
  97.125 109.125  94.125  79.625 124.75  103.75  109.5    98.75  110.25
 105.25  109.25  104.     76.125  94.375  76.75  112.5   112.875  97.75
  84.25   90.75   72.375  99.375  90.     84.     87.375 109.75   99.625
 122.375  74.875  93.625 100.25   83.625  96.625  87.5    99.625  85.5
 106.75   86.75   99.5    92.625 121.75  104.375  83.625  92.375  94.
  80.25   89.     97.625  95.25  100.25  112.625 105.875 105.25   97.875
 100.5    93.125 103.375 112.75   86.     84.875  90.75  128.875]
indices to prune: 0
cascading layers [BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False), Conv2d(512, 512, kernel_size=
(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False), Conv2d(
512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
Pruning 0 units from Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)) (out)
(0): Conv2d torch.Size([60, 22, 4, 4])
(1): ReLU None
(2): BatchNorm2d torch.Size([60])
(3): Conv2d torch.Size([128, 60, 4, 4])
(4): ReLU None
(5): BatchNorm2d torch.Size([128])
(6): Conv2d torch.Size([256, 128, 4, 4])
(7): ReLU None
(8): BatchNorm2d torch.Size([256])
(9): Conv2d torch.Size([512, 256, 4, 4])
(10): ReLU None
(11): BatchNorm2d torch.Size([512])
(12): Conv2d torch.Size([512, 512, 3, 3])
(13): ReLU None
(14): BatchNorm2d torch.Size([512])
(15): Conv2d torch.Size([512, 512, 3, 3])
(16): ReLU None
interest layer num: 8
0
1
2
[ 98.625  81.75   88.125  93.125  83.75   96.125  99.375  96.625 104.25
  96.125 105.     99.875  92.625  96.     95.875 103.25   83.5    97.5
  80.75   85.875  97.5    99.875  88.     94.    112.625  92.375 100.
 108.    105.    100.875  94.375  88.375  87.625  94.75   95.125 100.875
  86.125  94.25   99.875  93.625  87.125  88.625  96.625 100.375  84.125
  88.125  99.25   91.625 107.375  96.875  95.625  90.375  89.75   98.375
  99.125  93.875  82.25  101.625 107.875 101.625  78.75   82.     92.875
  94.25   86.375  91.125 112.125  99.75  119.5    89.25   93.125 112.375
  98.875  94.625 110.5   108.5   100.5    92.5    88.625  99.25   82.25
  92.875 101.625  87.    100.875  88.875  91.875  90.125  92.75   86.875
  83.375  75.875  93.625  95.    107.625  94.625  78.25   97.75   96.25
  95.75  103.     94.25  112.875 100.     92.     97.375  99.    101.
 103.    103.375  90.625 106.     96.625  90.375  98.25  102.625  96.
 103.25   94.625  92.625  95.25   94.25   97.    106.5    78.875  97.875
  82.125 101.    114.5    83.625  98.625  89.75  106.75   91.625  81.5
  92.5    91.625  99.75   92.125  99.375 111.25   98.375 102.375  91.
  88.875  92.     91.5   106.125  95.25   93.5   100.5   101.     96.75
  97.75   88.875  95.5    92.125  96.25  105.75   98.375 106.     93.5
  91.625  94.5    96.375  98.125  81.     99.75  104.875  93.375  91.375
  99.125  90.875  85.375  85.5   101.75   92.5   100.5    99.125  99.75
 109.875  99.    108.875  88.625  91.875  89.25  101.375 106.25  113.125
  95.75   91.875 102.75  101.125  85.625  91.5    80.25   97.    107.625
 100.625 104.75   88.625  98.5    88.25   89.     88.    103.125 101.5
  91.375  98.875 101.375  98.125  90.125 102.5   106.125  96.875  93.375
  85.875  97.875 106.125  87.125  77.5    88.5    97.875 101.625 109.375
  93.125 110.5   105.875  99.25   88.625  82.125  99.375  95.875 109.25
  99.125 105.25   97.     91.25   96.625 113.125  91.     89.     98.75
  97.625  90.75   94.25   96.5   103.875  93.125  85.625  89.75   87.5
  95.75   97.625  95.125  79.125 103.5    84.125  78.875  84.5    98.625
 114.125 100.875  86.25   89.25   90.25   93.375  91.875  98.625  90.125
  88.375 118.5   102.     98.625 103.    100.    110.     95.25  105.125
 105.375  93.375 107.875 102.     87.25  104.25   81.75  107.     98.875
  94.125  98.75   94.375 107.5   100.125  93.25  107.5    86.25   91.125
  95.875  98.625  92.625  91.25   94.125 102.5    88.125 108.125  95.
  94.25  101.5    99.5    99.125  90.125  93.125  99.5    86.875  98.875
  97.5    98.25  101.25  113.5   104.75   98.25   97.     89.375  94.5
  94.125 116.875 110.625  92.625  87.875 106.375 101.5   103.     93.125
 105.875  88.875 104.75  109.     89.     97.5    98.     99.75   92.
  92.625 111.5    92.375  84.875  83.    101.375 108.     95.     83.875
  93.875  93.75   97.625  88.     87.875  99.25  114.75   95.75   90.5
 103.25   91.625  93.25   88.25  104.875 104.5    96.25   89.75   98.375
  96.125  87.125 100.75   99.125  88.625  73.625  92.625  86.625 101.5
  81.75   97.125 103.     95.75  104.75  102.     91.25   80.625  93.375
  77.875  95.125 101.5    97.125  93.25  108.625  81.5   115.625 106.875
  95.625  91.125  99.75   92.5   118.5   109.     92.75   93.    103.25
  96.125 106.875 109.     92.25   87.5    92.     91.25   79.625  94.625
  78.5    92.75   77.375  99.375  98.5    92.5    98.    103.     92.125
 100.25   97.    105.5   108.875 104.625  97.125  99.625  97.75   98.625
  87.875  98.875  81.875  93.    103.25   96.25   81.     91.75   99.75
  85.75  112.5   100.375  87.875  83.875  87.25   90.75   97.875 111.125
 117.375  91.    101.625  94.125  92.375  87.     84.5   116.625 100.375
  96.625 111.625  95.25   78.5   108.625  90.25  105.5    90.     93.375
 107.    102.25   93.375 101.875  98.125  97.     93.     86.375  90.125
  85.875 113.25  107.    105.5   100.75   87.625 101.875  90.25   88.75
  85.625 107.5    91.5    96.125  88.     85.25  103.75  114.25   98.25
  94.5    94.25   94.625  79.25   88.25   80.75   90.375  91.5    99.25
  91.25   92.875  94.625  77.125 102.125  78.125  89.5    93.375]
indices to prune: 0
cascading layers [BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False), Conv2d(512, 512, kernel_size=
(3, 3), stride=(1, 1), padding=(1, 1))]
Pruning 0 units from Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (out)
(0): Conv2d torch.Size([60, 22, 4, 4])
(1): ReLU None
(2): BatchNorm2d torch.Size([60])
(3): Conv2d torch.Size([128, 60, 4, 4])
(4): ReLU None
(5): BatchNorm2d torch.Size([128])
(6): Conv2d torch.Size([256, 128, 4, 4])
(7): ReLU None
(8): BatchNorm2d torch.Size([256])
(9): Conv2d torch.Size([512, 256, 4, 4])
(10): ReLU None
(11): BatchNorm2d torch.Size([512])
(12): Conv2d torch.Size([512, 512, 3, 3])
(13): ReLU None
(14): BatchNorm2d torch.Size([512])
(15): Conv2d torch.Size([512, 512, 3, 3])
(16): ReLU None
interest layer num: 0
0
1
2
[0.0000000e+00 0.0000000e+00 1.2287750e+04 0.0000000e+00 5.9702500e+03
 2.3025000e+02 0.0000000e+00 6.2151250e+03 0.0000000e+00 6.2786250e+03
 1.2284750e+04 1.2287250e+04 0.0000000e+00 0.0000000e+00 1.8725000e+02
 6.0498750e+03 1.2288000e+04 1.2288000e+04 5.8528750e+03 1.2152750e+04
 7.9625000e+01 0.0000000e+00 8.1000000e+01 1.2288000e+04 0.0000000e+00
 7.8750000e+01 2.0125000e+01 1.2288000e+04 1.2284375e+04 1.2288000e+04
 0.0000000e+00 1.2288000e+04 1.2288000e+04 1.2288000e+04 8.8125000e+01
 0.0000000e+00 6.3936250e+03 0.0000000e+00 1.2287500e+04 1.2288000e+04
 0.0000000e+00 6.1710000e+03 6.1351250e+03 0.0000000e+00 0.0000000e+00
 1.2287875e+04 1.2288000e+04 0.0000000e+00 0.0000000e+00 6.5553750e+03
 1.2282875e+04 1.2500000e-01 1.2238250e+04 0.0000000e+00 5.9602500e+03
 0.0000000e+00 3.9750000e+01 0.0000000e+00 1.2288000e+04 6.0733750e+03
 1.0250000e+01 0.0000000e+00 2.8537500e+02 1.2288000e+04]
indices to prune: 21
cascading layers [BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False), Conv2d(64, 128, kernel_size=(4
, 4), stride=(2, 2), padding=(1, 1)), BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False), Conv2d(12
8, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)), BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_s
tats=False), Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)), BatchNorm2d(512, eps=1e-05, momentum=0.1, affine
=True, track_running_stats=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(512, eps=1e-05
, momentum=0.1, affine=True, track_running_stats=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
Pruning 21 units from BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False) (in)
Pruning 21 units from Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)) (in)
Pruning 21 units from Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)) (out)
(0): Conv2d torch.Size([43, 1, 4, 4])
(1): ReLU None
(2): BatchNorm2d torch.Size([43])
(3): Conv2d torch.Size([128, 43, 4, 4])
(4): ReLU None
(5): BatchNorm2d torch.Size([128])
(6): Conv2d torch.Size([256, 128, 4, 4])
(7): ReLU None
(8): BatchNorm2d torch.Size([256])
(9): Conv2d torch.Size([512, 256, 4, 4])
(10): ReLU None
(11): BatchNorm2d torch.Size([512])
(12): Conv2d torch.Size([512, 512, 3, 3])
(13): ReLU None
(14): BatchNorm2d torch.Size([512])
(15): Conv2d torch.Size([512, 512, 3, 3])
(16): ReLU None
interest layer num: 2
0
1
2
[1346.75  1284.625 1734.125 1335.375 1763.625 1769.25  1710.125 1710.75
 1703.    1810.875 1325.25  1299.375 1830.    1255.25  1266.625 1737.75
 1338.    1691.75  1190.25  1728.125 1346.875 1788.75  1352.875 1329.625
 1708.5   1398.25  1803.375 1747.5    309.25  1822.5   1749.5   1746.5
 1710.625 1301.625 1827.5   1182.75  1350.25  1704.25  1736.125 1357.625
 1339.    1741.875 1849.75  1212.125 1786.75  1294.5   1745.    1761.5
 1709.875 1731.125 1744.75  1353.875 1230.875 1284.25  1271.125 1728.75
 1745.75  1344.375 1771.625 1332.    1748.125 1223.    1311.    1289.375
 1336.125 1269.    1325.375 1744.    1221.5   1372.    1333.25  1318.875
 1246.375 1310.875 1790.25  1796.75  1738.125 1854.375 1339.5   1379.875
 1246.75  1220.75  1762.5   1776.25  1301.5   1703.875 1350.375 1307.75
 1876.25  1347.25  1814.5   1159.125 1331.125 1366.375 1873.    1752.875
 1353.375 1727.25  1378.625 1743.875 1856.75  1698.375 1770.125 1810.75
 1732.25  1351.375 1358.75  1354.25  1366.625 1736.75  1365.625 1308.25
 1255.75  1725.5   1802.    1143.375 1716.    1775.625 1306.25  1716.25
 2620.875 1788.    1289.875 1245.125 1359.625 1331.75  1727.625 1264.75 ]
indices to prune: 0
cascading layers [BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False), Conv2d(128, 256, kernel_size=
(4, 4), stride=(2, 2), padding=(1, 1)), BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False), Conv2d(
256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)), BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running
_stats=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(512, eps=1e-05, momentum=0.1, affi
ne=True, track_running_stats=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
Pruning 0 units from Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)) (out)
(0): Conv2d torch.Size([43, 1, 4, 4])
(1): ReLU None
(2): BatchNorm2d torch.Size([43])
(3): Conv2d torch.Size([128, 43, 4, 4])
(4): ReLU None
(5): BatchNorm2d torch.Size([128])
(6): Conv2d torch.Size([256, 128, 4, 4])
(7): ReLU None
(8): BatchNorm2d torch.Size([256])
(9): Conv2d torch.Size([512, 256, 4, 4])
(10): ReLU None
(11): BatchNorm2d torch.Size([512])
(12): Conv2d torch.Size([512, 512, 3, 3])
(13): ReLU None
(14): BatchNorm2d torch.Size([512])
(15): Conv2d torch.Size([512, 512, 3, 3])
(16): ReLU None
interest layer num: 4
0
1
2
[336.    294.375 418.5   443.75  249.375 422.    418.    414.5   294.625
 403.25  310.75  289.75  477.75  330.875 333.625 283.625 437.5   445.25
 474.25  476.    464.125 325.625 351.125 427.75  321.5   413.5   333.875
 210.875 453.5   444.75  288.625 261.5   319.625 531.25  425.875 456.75
 320.125 282.25  456.125 496.125 304.5   342.25  323.75  312.75  418.5
 314.375 318.375 354.5   305.75  452.875 272.375 434.375 610.125 452.625
 300.625 334.125 309.75  384.75  436.5   467.5   281.75  312.5   342.
 570.125 197.875 248.625 472.875 318.875 315.25  459.75  446.125 474.25
 424.5   291.125 285.25  426.625 315.    523.5   317.875 412.75  344.375
 343.5   481.375 357.25  422.5   344.375 437.625 426.125 415.    456.5
 174.    466.875 522.    321.875 448.625 275.    471.125 448.    467.
 456.625 461.875 330.625 455.125 447.125 479.125 313.75  494.625 441.875
 460.25  429.375 314.625 306.875 312.875 449.25  208.875 451.25  407.75
 564.625 332.    318.25  292.375 329.5   331.25  438.    310.    301.25
 542.    550.    333.75  459.125 465.875 562.125 444.875 301.    323.5
 474.875 332.    301.125 450.625 333.5   473.5   434.375 261.    280.5
 463.125 448.75  228.625 298.625 283.875 291.5   421.125 428.75  334.
 311.    445.125 294.875 334.625 258.    474.    293.875 545.    357.625
 502.875 336.5   274.5   305.125 450.875 338.75  347.    579.875 335.75
 472.75  326.25  221.875 325.125 315.    320.125 449.625 460.25  552.5
 284.    484.    322.875 336.125 478.75  457.25  437.    446.875 291.
 281.25  472.375 192.5   448.875 431.5   304.625 510.375 420.75  310.25
 485.125 326.75  288.    453.25  431.25  327.375 346.75  441.75  433.5
 433.75  349.125 477.625 213.375 286.375 334.375 335.75  350.    295.875
 429.375 200.125 451.25  359.5   230.    452.875 435.125 335.5   482.
 552.25  331.75  461.5   336.75  351.875 465.25  454.625 458.75  347.125
 328.25  354.    246.375 188.125 420.625 446.875 489.5   313.5   451.75
 453.375 586.625 468.75  505.25  195.5   406.    430.25  424.875 338.
 345.375 451.5   433.375 432.875]
indices to prune: 0
cascading layers [BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False), Conv2d(256, 512, kernel_size=
(4, 4), stride=(2, 2), padding=(1, 1)), BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False), Conv2d(
512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running
_stats=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
Pruning 0 units from Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)) (out)
(0): Conv2d torch.Size([43, 1, 4, 4])
(1): ReLU None
(2): BatchNorm2d torch.Size([43])
(3): Conv2d torch.Size([128, 43, 4, 4])
(4): ReLU None
(5): BatchNorm2d torch.Size([128])
(6): Conv2d torch.Size([256, 128, 4, 4])
(7): ReLU None
(8): BatchNorm2d torch.Size([256])
(9): Conv2d torch.Size([512, 256, 4, 4])
(10): ReLU None
(11): BatchNorm2d torch.Size([512])
(12): Conv2d torch.Size([512, 512, 3, 3])
(13): ReLU None
(14): BatchNorm2d torch.Size([512])
(15): Conv2d torch.Size([512, 512, 3, 3])
(16): ReLU None
interest layer num: 6
0
1
2
[106.75   84.75   90.625  75.25  107.875 100.875  98.75  100.875  97.25
  97.25   88.125  77.125 100.75  110.625  96.75   96.375  95.375 112.
  98.375 113.875 110.    102.875  75.5    94.875 107.5    77.875 112.375
  85.125 113.875  92.625  80.375 106.5    91.    119.25   69.875 106.5
 107.125 106.875  68.875  72.75   79.125  83.75   89.5    76.5   101.5
  95.25   74.5   100.25  108.5   105.125  66.75   71.25  114.125  89.375
  86.25  105.25  104.375  88.5   100.625 100.125 103.     75.75  102.75
  87.25  117.    116.125 109.75  104.5    87.875  89.    108.     81.125
  86.75  107.25   85.625 112.    126.875  91.5   112.125 103.625  88.375
  77.75  101.25   95.5   105.25  118.125 101.625  97.     90.25  104.25
  88.25   82.75   86.125 114.625 112.125  89.625  84.75   80.625  84.375
 119.625  88.    111.625  78.625 106.375 121.375  92.25   90.5    97.625
  82.25   78.    103.75   66.     85.875  74.625  96.375 104.25   79.75
  98.875  88.875  99.25  101.625  64.125  78.875 118.5   107.25   71.
 103.25   76.     89.5   119.25   95.5    80.875 109.5    73.875 107.375
  83.375  87.625 101.5   108.125  98.375  81.75   90.625 122.75  110.375
 114.875 103.375 105.5   114.75   96.125  97.375 104.125 104.875 124.
  76.875  99.25   82.375  83.625  90.25  112.875  91.125 118.875 104.125
 104.75  103.75  102.75   72.625 104.125  97.75  101.625  92.    121.375
 123.25   94.375 109.25   87.875 123.125 107.75  109.75   89.375 107.25
 119.125 115.375 103.125  89.875  82.75  114.25   96.25   82.25   80.875
  93.75  111.375 123.125  99.125 106.75   99.     95.75   68.625  88.375
  96.375  99.5    84.625  66.875 123.875  94.25   97.5    78.125  77.25
  80.75   73.125 117.625  91.75  111.25  101.875 105.25  104.625  83.5
  95.875 103.5   106.875 113.5    82.5   111.625 109.375  73.25   82.875
 122.75   96.75   93.875 102.5   107.5    88.75   98.125  90.    104.75
 120.25  107.875 103.     75.25  115.25   83.25   83.25   89.75   77.5
  83.375  98.375  89.5   119.375  85.     77.75   89.125 100.625 108.125
 113.75   96.125 109.     99.5    97.875 105.375  97.5   116.5   117.
  91.125  79.375 116.375  79.625 121.625  91.     81.     92.    116.75
  94.    120.25  105.375 106.75  105.125 110.625  95.375  61.5    94.125
  94.625  85.     84.75  117.625  99.625 101.625  94.375 101.25   98.125
  80.75   81.75   99.     98.75  102.625  87.75  102.5    77.75   99.5
 101.375 121.875 110.5    72.625  91.875 100.875 129.5    94.875 123.
 110.    114.125  79.75  101.125 119.5    92.625 114.625  96.25   92.
  94.75  104.375 123.875  80.5    95.625 104.125 116.375  90.     88.75
  88.375 115.875 119.375  82.875  80.    106.25  101.375  81.5   104.
  97.     67.375  87.75  103.25   85.125  93.125  86.875  75.375  91.875
  79.875  89.75  114.25   67.625  85.625  88.125  75.5    92.25  111.75
 119.25  121.375  81.5    97.125 102.125 105.625 107.625 109.125  85.75
  80.25   76.     99.     99.125  77.75  119.    111.75   85.75  103.625
  92.75   79.5    87.5   119.75  110.75  117.625 102.5   103.375  96.875
 104.25   89.625  86.125  93.625  93.625  96.625 104.375 114.875  88.625
  87.375  85.125 106.5    98.25   90.     87.625  95.75   72.125  86.5
  75.75   73.875  70.75  110.625  70.5    69.375 117.375  91.25  129.25
  84.625  66.75   90.375  88.75  107.5    81.375  88.625 104.    108.625
  98.625 113.75  111.     85.75  102.125  84.125  99.625 108.75   98.75
  69.75  118.25   96.5   105.625  93.625 101.25  104.625 105.875 113.125
  95.    117.875 125.75   83.875 109.375 103.875 118.5   108.     65.125
  81.625 104.75  106.5    80.875  70.     89.875  95.75   91.75  110.25
  93.75   88.75   98.375  67.5   105.75   79.375 124.75   85.75  101.125
  92.875  65.375  85.375  88.625  90.5   126.    100.625 104.875  85.125
  81.625 122.5   114.5    90.5    84.625  94.5    83.125 115.     88.25
  95.375  97.625  85.125 108.25   99.75   87.25   87.875 117.875  97.75
 113.875 125.625  87.75  118.625  99.75  105.875  87.625 105.875  77.625
  91.75   89.625 112.     68.375  99.125  86.75  100.625  75.    121.
 114.375  96.375 128.875  89.5    67.75   93.125  72.375  67.125]
indices to prune: 0
cascading layers [BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False), Conv2d(512, 512, kernel_size=
(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False), Conv2d(
512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
Pruning 0 units from Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)) (out)
(0): Conv2d torch.Size([43, 1, 4, 4])
(1): ReLU None
(2): BatchNorm2d torch.Size([43])
(3): Conv2d torch.Size([128, 43, 4, 4])
(4): ReLU None
(5): BatchNorm2d torch.Size([128])
(6): Conv2d torch.Size([256, 128, 4, 4])
(7): ReLU None
(8): BatchNorm2d torch.Size([256])
(9): Conv2d torch.Size([512, 256, 4, 4])
(10): ReLU None
(11): BatchNorm2d torch.Size([512])
(12): Conv2d torch.Size([512, 512, 3, 3])
(13): ReLU None
(14): BatchNorm2d torch.Size([512])
(15): Conv2d torch.Size([512, 512, 3, 3])
(16): ReLU None
interest layer num: 8
0
1
2
[ 97.    102.    107.625  86.5    90.25   90.875  85.5   102.375  95.
 101.125  95.125  93.    101.5    87.75  104.5    95.25   85.25   86.75
 103.875 102.     84.5    84.25  105.375 100.375 107.75   97.     85.75
  93.625 102.5   108.75   83.5    90.125  86.125  98.25   83.     90.125
 107.25  102.625 104.5    90.5   102.5   103.125 102.    103.5    88.625
  88.625  97.875  99.625 103.5   101.625  91.     89.625 104.75  102.5
  91.5   107.25  107.     93.875  96.     99.5    94.375  98.625 100.5
  96.875 112.125 109.75   84.25   89.     91.375  93.25   83.625 102.875
  87.5   100.875  90.125 101.375 113.125  90.625  83.875  89.875  97.625
  92.75  101.    101.75  107.625  98.125 106.625 106.375 102.125 101.125
  90.875  95.625 100.75   90.     95.75   81.125  95.5   103.125 104.375
  89.    111.375 100.75   87.125 102.5    97.5    91.875  87.875 105.75
  87.625 106.125 105.875 103.125 107.625 110.25   92.875  98.875  96.25
 101.125  96.375  97.875  89.75   93.125  93.     88.125 104.875  84.875
 102.25   90.25   84.125  99.5    89.     88.125 106.875  90.     86.25
  96.875 107.     88.75   83.125 100.75   96.125  96.75   83.75   98.
  90.25  103.125  97.625 105.75  108.625 106.625 105.75  106.25   89.75
  90.125  88.75   94.875 107.125 100.125 107.    110.25   88.75  101.125
 103.125  91.75   98.625  98.    104.     96.25   87.25   91.125 102.75
 102.625  93.     98.625  93.    104.25  100.5    92.5    97.5    91.875
  87.875 104.25   87.875 102.    103.    106.5    93.75  102.25   84.875
 105.625  84.75   95.5    83.625  90.875  90.25   94.5    88.875 104.75
  90.75  105.75   88.     82.     95.    111.5    90.625  85.75   94.
  92.75  105.75  105.125  91.125  92.125  97.875 106.25   92.5   100.25
  90.     89.25   85.875 107.375 106.375  88.875  93.5    86.875 100.125
 106.625  87.625  89.875  94.75   98.5    91.875 105.625  84.625  83.5
 106.375  89.625  95.625  84.875 102.25   89.375  88.5   101.125 101.25
  89.125  88.875  89.375  86.625  92.25  101.125  97.125 106.625 105.
  86.875 100.25   98.5    87.125  78.75   88.     86.5    91.375  82.125
  94.625  90.5   101.875 103.     89.75  104.     89.125 107.125  88.625
 105.     84.75  104.125  98.125 103.5    86.25   95.5    84.625  84.625
  90.875  85.    106.125 103.625  85.25   87.625 102.75  102.75   92.75
 101.75   92.75   97.625  87.5    83.125 101.75   97.375  93.125 104.
  87.375  89.875  97.     84.375  87.    100.25  102.875  88.875 100.25
  96.625  98.5    81.125 105.     99.75   85.875 103.75   90.625  99.625
  85.875 102.25   84.875  85.5    95.125 101.625  94.125 103.5   103.75
  89.625 102.25  104.5   104.375  90.5   100.125  89.375  99.375 103.75
  95.625 100.     94.25  101.875 108.625  90.5   104.5   101.    109.
  90.25   93.75   96.875 109.625  87.75  104.375  92.5    85.375  97.75
  92.875 106.75   88.5   101.25   90.5    89.25  103.625  97.25   90.75
  91.625  99.125 104.375 110.5   105.75  110.75   86.25   97.125  80.875
  98.375 103.125 102.375  90.    107.75  100.625  95.375 102.5    97.25
  86.     91.    106.75  100.875  85.5    85.375 107.375 102.875  92.5
  91.375  83.375  94.125  88.625  88.5    90.25   89.375 103.375 105.5
  90.625  90.75   93.875  87.125  92.75  106.25  109.875  85.125  99.125
  88.25   95.5    98.5   105.     82.     85.75  107.875 103.     99.875
  99.875  93.25   87.5   102.75  108.75  102.25  100.25  105.5    90.75
  88.75  105.25   84.     93.875 107.375 102.75   87.875 108.25  104.75
  90.375  87.375 112.125  91.25   87.375  93.25   97.5    86.125 107.625
  89.375  86.5    90.375  96.625 101.25  103.875 102.875 105.75  103.375
 100.625  88.625 104.75   97.5   104.25   98.875  98.625  89.375  85.75
  96.875 105.375 103.25   91.625 106.     87.    106.75   84.375  83.875
  88.625 102.     86.25   91.75   92.625  87.25  102.375  86.625 106.875
 113.875  90.875  99.625  92.75  101.5    92.125  97.     80.25   85.75
 103.125 109.5   105.25   91.    101.875 104.5    96.5    91.875  92.25
  95.375 103.875 108.375  87.875  96.125  98.125  86.375  87.25  103.125
  93.25   91.25   87.5    97.    101.875 114.125  86.875  84.75 ]
indices to prune: 0
cascading layers [BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False), Conv2d(512, 512, kernel_size=
(3, 3), stride=(1, 1), padding=(1, 1))]
Pruning 0 units from Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (out)
(0): Conv2d torch.Size([43, 1, 4, 4])
(1): ReLU None
(2): BatchNorm2d torch.Size([43])
(3): Conv2d torch.Size([128, 43, 4, 4])
(4): ReLU None
(5): BatchNorm2d torch.Size([128])
(6): Conv2d torch.Size([256, 128, 4, 4])
(7): ReLU None
(8): BatchNorm2d torch.Size([256])
(9): Conv2d torch.Size([512, 256, 4, 4])
(10): ReLU None
(11): BatchNorm2d torch.Size([512])
(12): Conv2d torch.Size([512, 512, 3, 3])
(13): ReLU None
(14): BatchNorm2d torch.Size([512])
(15): Conv2d torch.Size([512, 512, 3, 3])
(16): ReLU None
interest layer num: 0
0
1
2
[ 0.     6.    43.    47.5   48.     5.    48.     0.375 10.    48.
 47.5    4.375 35.5    1.625  9.375  5.375 26.5   19.     8.125 48.
  0.    44.     0.875  0.375  0.    18.375 42.    47.625 38.    36.25
 47.125 48.     4.5    0.     1.875 47.25   5.    45.375  0.     4.
 47.     0.25  48.    39.25   2.375  0.375 46.75  36.75  48.    12.875
  0.     0.     2.375  1.375 44.     0.    41.625  0.625 13.25  48.
 48.    46.875 45.875  0.    39.75   7.75  18.125  0.     0.875 19.625
  9.875 48.    47.75  48.     0.125 46.625  0.     0.    40.625 48.
  0.625  2.875 48.    48.    35.125 41.    46.875 37.25  20.    47.
  1.     4.25  26.75   6.125 11.875  0.     3.75  20.75  40.     0.25
 26.125 34.5   41.25  43.625 37.375 40.    47.    36.125  0.    44.125
  9.875 48.     6.875  0.    43.125  8.125  4.25   8.5    1.    48.
  0.     6.     3.    45.5    0.    41.875 48.     0.     2.5    0.125
 43.25  46.125 21.625 40.5   14.75  19.25   1.    47.    48.     0.
 24.375  0.375 46.5   48.     0.    43.375  5.75   2.25  45.75  48.
  6.375 48.     0.125  8.125  9.75  45.5    1.375 36.125  0.    12.25
  4.875 41.75   0.    47.375 41.5    6.     0.    11.875  0.    13.875
 33.     0.375 48.    47.    29.125  0.    48.     9.25  35.125  0.625
 47.125  0.    37.     6.25  38.75  43.     0.125  1.    46.875 40.875
 48.    48.     0.    27.     0.    31.625 44.    37.875 35.    18.
 40.625 10.875 48.    34.875 10.75   1.375 42.     0.    48.    48.
  3.125  0.     8.     0.25  41.    24.25   0.     0.125 13.25   8.
 48.    43.875 47.875 19.     0.    47.875 40.5   48.    38.125  0.
 39.625 44.375 28.75  35.125 15.25   0.    47.    45.    12.625  0.
  8.    38.25  36.125 19.125  8.125 34.     8.125 46.875  0.75   0.
  1.375 10.5   48.     0.    43.5    0.    47.875 40.     7.5   48.
  0.    48.     0.     0.     6.    14.75   4.    48.    46.25   0.
 15.875  3.    13.625 13.    40.    27.75  48.    40.375 15.5   12.25
 46.125  0.5    0.     9.875 42.    46.375 41.75   1.     8.875 11.25
  0.    10.375  0.     4.125 43.25   0.    44.625  0.5   48.    16.625
  9.    46.25  37.     0.     0.     7.5    9.125 48.     0.    20.625
  7.75   2.75   0.625 48.    45.25   7.875 32.625  0.     9.25  47.25
  1.    44.    48.    48.     0.    37.    48.     0.     0.    48.
 47.5   21.375 48.     4.375  6.25   7.5   27.875  0.375  0.    39.375
 42.375 36.    48.    41.25  13.    47.625  9.875 48.    10.625 46.375
  0.    47.875 46.    41.25  36.    34.125  4.5    0.    23.125 48.
 47.    18.125  0.    48.     5.125 48.     0.5   48.    43.5    6.
  0.25  10.25  19.875 29.25  46.875  0.625 21.125  0.125 47.125 48.
  5.75  21.375 10.625  1.625 44.25  10.875 43.75   0.    47.875 11.375
 46.625  1.875 14.125  0.25  22.375 48.    48.     0.    22.875 35.5
  2.625  7.     0.    47.875 46.875 10.5    8.    40.75  38.375 48.
  2.25   7.75   6.     0.    40.5   47.25  35.875 47.625 37.625 30.125
 41.125 22.75   4.625 32.125 47.     0.    41.375  0.    16.25   0.875
 44.5   19.75   1.125 39.75   0.    48.    31.125  3.75  48.    25.25
 39.875 22.75   1.125  0.125 16.    39.25  48.    28.375  0.    14.625
  0.     0.     7.125 11.5    3.    11.75   6.25  40.375 26.75  23.25
 48.     0.125 10.625  2.125  8.875  0.    45.875 24.125 47.75  48.
 47.875  1.    48.     0.625  1.     0.     0.    25.25   0.     0.
  0.    48.     9.    44.25  48.    20.625 24.625 48.    47.25  17.125
  2.    46.25  47.625 16.875  0.875 42.    48.    46.     0.     1.
 41.625 48.    32.875 29.25   0.75  48.    39.875 40.625 48.     3.875
  0.    36.   ]
indices to prune: 75
cascading layers [BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False), Conv2d(512, 256, kernel_size=
(4, 4), stride=(2, 2), padding=(1, 1)), BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False), Conv2d(
256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running
_stats=False), Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine
=True, track_running_stats=False)]
Pruning 75 units from BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False) (in)
Pruning 75 units from Conv2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)) (in)
Pruning 75 units from Conv2d(192, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)) (out)
(0): Conv2d torch.Size([437, 192, 4, 4])
(1): BatchNorm2d torch.Size([437])
(2): ReLU None
(3): Conv2d torch.Size([256, 437, 4, 4])
(4): BatchNorm2d torch.Size([256])
(5): ReLU None
(6): Conv2d torch.Size([128, 256, 3, 3])
(7): BatchNorm2d torch.Size([128])
(8): ReLU None
(9): Conv2d torch.Size([64, 128, 3, 3])
(10): BatchNorm2d torch.Size([64])
(11): ReLU None
interest layer num: 2
0
1
2
[ 7.375  8.375  2.5    6.5    9.125 11.     3.375  6.375  4.75  10.375
  7.375  9.875  9.125  2.625  9.5    6.125  0.     2.     8.     5.75
  8.375  0.125  3.     9.75   7.625  3.375  3.625  7.25   9.75   4.75
  4.5    4.25  11.25   6.125 11.375  3.125  0.625  8.625  5.25   0.125
  2.25  12.    10.875  5.875  0.375  7.125  7.625  9.75   2.125  3.125
  7.75   9.5    6.375  9.75   4.75   1.125  2.875  1.75   2.375 11.125
  2.5   10.875  3.625  8.125 11.5    1.5    4.625  2.375 11.5    9.5
  8.625 10.     1.     7.25   8.25   3.     3.     0.875  4.125  9.
 10.125 11.125  9.375  7.125 11.75   6.875  7.25  10.     7.125  3.875
  7.     5.625  2.125  4.    10.5    0.375  0.25   4.5   10.375  3.875
  6.     3.25   8.625  6.     0.5    9.25   2.5    5.5   10.875  0.
  4.375  5.625  6.375  7.     9.     1.25   2.875  4.375 12.     4.125
  4.375  0.    10.75   6.     3.625  7.625 10.125  4.375  8.125  7.875
  3.    11.375  8.375  6.25   2.75   4.75   9.25  10.625  4.875  7.
  6.375  6.75  11.375  3.375  6.     3.5    5.375  4.5    1.25   4.125
  4.375  9.375  9.625  4.125  4.75   2.     6.125  7.625  3.125  0.375
  3.5    6.875 10.     2.625  6.125  2.875  2.875  8.625  7.125  8.75
  9.25   5.     7.5    5.25   4.625  4.25   7.25   9.75   1.625 10.5
  4.25   1.625  3.     9.25   4.875  2.875  6.75   1.75   0.125 11.875
  5.5   11.125  2.375  7.625  9.     8.625  6.625  7.125  7.     2.375
  5.     7.125 10.25   5.875  8.125  6.25  11.25   1.75   8.875  3.375
  8.5   10.375  1.75   4.5    6.125  3.375  8.125  7.125  4.625  1.375
  6.     5.625  1.125  6.5    9.375  4.375  1.125  5.875  4.25  10.375
 11.25   4.25   1.25   5.5    7.875 11.5   11.375 10.5    0.75   5.625
  7.375  8.875  2.375  3.625  4.125  2.625  3.75   4.75   9.5    6.625
  9.875  5.875  9.125  7.125  1.875  3.25 ]
indices to prune: 3
cascading layers [BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False), Conv2d(256, 128, kernel_size=
(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False), Conv2d(
128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_s
tats=False)]
Pruning 3 units from BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False) (in)
Pruning 3 units from Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (in)
Pruning 3 units from Conv2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)) (out)
(0): Conv2d torch.Size([437, 192, 4, 4])
(1): BatchNorm2d torch.Size([437])
(2): ReLU None
(3): Conv2d torch.Size([253, 437, 4, 4])
(4): BatchNorm2d torch.Size([253])
(5): ReLU None
(6): Conv2d torch.Size([128, 253, 3, 3])
(7): BatchNorm2d torch.Size([128])
(8): ReLU None
(9): Conv2d torch.Size([64, 128, 3, 3])
(10): BatchNorm2d torch.Size([64])
(11): ReLU None
interest layer num: 4
0
1
2
[ 9.     7.375  4.375  4.25   6.625  0.25   7.5    5.     6.125  4.375
  2.25   8.875  7.5    2.5    3.25   5.875  4.5    7.25   2.625  4.
  6.5    3.75   3.5    5.125  1.125  2.875  8.5    2.875  1.     3.875
  9.5    3.375  8.625  4.     6.25   7.     6.25   7.25   3.75   8.125
  2.75   6.375  3.125  4.75   0.625  8.625  6.125  8.75   6.875  8.
  9.125  1.75   6.25   2.75   6.75   5.25   5.125  3.875  4.625  8.25
  5.5    2.5    8.    11.125  5.375  8.5    9.     5.5    6.875  7.875
 11.125  6.5    5.75   7.375  7.125  2.625  5.125  6.875  4.125  2.
  4.625  5.5    5.625  3.5    3.25   5.5    6.375  4.875  4.875  6.75
  6.125  6.375  3.75   9.875  1.     5.25   8.625 10.875  4.125  3.125
  3.125  9.125  3.125  8.25  11.125  1.625  5.     5.75   0.375  7.
  6.125  3.375  7.375  4.875  9.5    5.25   4.125  9.125  0.75   6.75
  8.5    6.5    3.125  9.75   2.875  3.625  4.875  7.75 ]
indices to prune: 0
cascading layers [BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False), Conv2d(128, 64, kernel_size=(
3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)]
Pruning 0 units from Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (out)
(0): Conv2d torch.Size([437, 192, 4, 4])
(1): BatchNorm2d torch.Size([437])
(2): ReLU None
(3): Conv2d torch.Size([253, 437, 4, 4])
(4): BatchNorm2d torch.Size([253])
(5): ReLU None
(6): Conv2d torch.Size([128, 253, 3, 3])
(7): BatchNorm2d torch.Size([128])
(8): ReLU None
(9): Conv2d torch.Size([64, 128, 3, 3])
(10): BatchNorm2d torch.Size([64])
(11): ReLU None
(0): Conv2d torch.Size([437, 192, 4, 4])
(1): BatchNorm2d torch.Size([437])
(2): ReLU None
(3): Conv2d torch.Size([253, 437, 4, 4])
(4): BatchNorm2d torch.Size([253])
(5): ReLU None
(6): Conv2d torch.Size([128, 253, 3, 3])
(7): BatchNorm2d torch.Size([128])
(8): ReLU None
(9): Conv2d torch.Size([64, 128, 3, 3])
(10): BatchNorm2d torch.Size([64])
(11): ReLU None
Finished training GMM, named: GMM!
gerashabanets@cp-vton-plus-vm-vm:~/CS231N-Pruning$
