{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pruning Untrained Networks\n",
    "This notebook shows that removing units with negative attributions can boost the accuracy of untrained networks on simple datasets significantly beyond random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append(\"./..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchsummary import summary, torchsummary\n",
    "from torchpruner import (Pruner, ShapleyAttributionMetric)\n",
    "import experiments.models.mnist as mnist\n",
    "import experiments.models.cifar10 as cifar10\n",
    "from experiments.utils import train, test\n",
    "\n",
    "# Fix seed for reproducibility\n",
    "# Since we do not perform any training, the accuracy after pruning is largely \n",
    "# affected by the network initialization (i.e. whether the randomly initialized \n",
    "# network contains good sub-graphs for the task).\n",
    "# While different seeds might give quite different results, in all cases\n",
    "# an increase in accuracy should be observed.\n",
    "torch.manual_seed(1)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(1)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print (f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST\n",
    "**Load a simple dense network with 2 hidden layers. 5.7M parameters and initial test accuracy ~10%**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "           Flatten-1                  [-1, 784]               0\n",
      "            Linear-2                 [-1, 2024]       1,588,840\n",
      "         LeakyReLU-3                 [-1, 2024]               0\n",
      "            Linear-4                 [-1, 2024]       4,098,600\n",
      "         LeakyReLU-5                 [-1, 2024]               0\n",
      "            Linear-6                   [-1, 10]          20,250\n",
      "================================================================\n",
      "Total params: 5,707,690\n",
      "Trainable params: 5,707,690\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.07\n",
      "Params size (MB): 21.77\n",
      "Estimated Total Size (MB): 21.84\n",
      "----------------------------------------------------------------\n",
      "Test set: Average loss: 2.3018, Accuracy: 716/10000 (7.160%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Load dataset\n",
    "train_loader, val_loader, test_loader = mnist.get_dataset_and_loaders(val_split=1000, val_batch_size=1000)\n",
    "loss = mnist.loss\n",
    "input_size = (1, 28, 28)\n",
    "\n",
    "# Print layer architecture and test performance\n",
    "model, name = mnist.get_fc_model_with_name()\n",
    "model.to(device)\n",
    "summary(model, input_size=input_size, device=device.type)\n",
    "test(model, device, loss, test_loader);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prune hidden layers using Shapley Value attributions.\n",
    "\n",
    "**The resulting network has ~41% of the original parameters and ~50% accuracy.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Shapley values on Linear(in_features=2024, out_features=2024, bias=True)...\n",
      "--> can run with partials\n",
      "Considering cascading modules [Linear(in_features=2024, out_features=10, bias=True)]\n",
      "Pruning 998 units from Linear(in_features=2024, out_features=10, bias=True) (in)\n",
      "Pruning 998 units from Linear(in_features=2024, out_features=2024, bias=True) (out)\n",
      "Computing Shapley values on Linear(in_features=784, out_features=2024, bias=True)...\n",
      "--> can run with partials\n",
      "Considering cascading modules [Linear(in_features=2024, out_features=2024, bias=True)]\n",
      "Pruning 693 units from Linear(in_features=2024, out_features=2024, bias=True) (in)\n",
      "Pruning 693 units from Linear(in_features=784, out_features=2024, bias=True) (out)\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "           Flatten-1                  [-1, 784]               0\n",
      "            Linear-2                 [-1, 1331]       1,044,835\n",
      "         LeakyReLU-3                 [-1, 1331]               0\n",
      "            Linear-4                 [-1, 1026]       1,366,632\n",
      "         LeakyReLU-5                 [-1, 1026]               0\n",
      "            Linear-6                   [-1, 10]          10,270\n",
      "================================================================\n",
      "Total params: 2,421,737\n",
      "Trainable params: 2,421,737\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.04\n",
      "Params size (MB): 9.24\n",
      "Estimated Total Size (MB): 9.28\n",
      "----------------------------------------------------------------\n",
      "Test set: Average loss: 2.2531, Accuracy: 5094/10000 (50.940%)\n",
      "\n",
      "CPU times: user 23.3 s, sys: 3.83 s, total: 27.1 s\n",
      "Wall time: 28 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Define prunable layers\n",
    "layers = list(model.fc.children())\n",
    "prunable_layers = [\n",
    "    # (module_to_prune -> [modules_for_cascading_pruning])\n",
    "    (layers[1], [layers[3]]),\n",
    "    (layers[3], [layers[5]]),\n",
    "]\n",
    "\n",
    "pruner = Pruner(model, input_size, device)\n",
    "attribution = ShapleyAttributionMetric(model, val_loader, mnist.loss, device, sv_samples=5)\n",
    "    \n",
    "# Prune layers starting from the outermost\n",
    "for module, cascading_modules in prunable_layers[::-1]:\n",
    "    # Compute Shapley Value attributions\n",
    "    attr, _ = attribution.run([module])[0]\n",
    "    # Select indices corresponding to negative attributions\n",
    "    pruning_indices = np.argwhere(attr < 0).flatten()\n",
    "    # Perform pruning\n",
    "    pruner.prune_model(module, pruning_indices, cascading_modules=cascading_modules)\n",
    "\n",
    "# Test final model\n",
    "summary(model, input_size=input_size, device=device.type)\n",
    "test(model, device, loss, test_loader);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-10\n",
    "**Same experiment on CIFAR-10. We use the same pruning procedure and network architecture (except the first layer now is equipped with 32x32x3 input units).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "           Flatten-1                 [-1, 3072]               0\n",
      "            Linear-2                 [-1, 2024]       6,219,752\n",
      "         LeakyReLU-3                 [-1, 2024]               0\n",
      "            Linear-4                 [-1, 2024]       4,098,600\n",
      "         LeakyReLU-5                 [-1, 2024]               0\n",
      "            Linear-6                   [-1, 10]          20,250\n",
      "================================================================\n",
      "Total params: 10,338,602\n",
      "Trainable params: 10,338,602\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.09\n",
      "Params size (MB): 39.44\n",
      "Estimated Total Size (MB): 39.54\n",
      "----------------------------------------------------------------\n",
      "Test set: Average loss: 2.3085, Accuracy: 1099/10000 (10.990%)\n",
      "\n",
      "Computing Shapley values on Linear(in_features=2024, out_features=2024, bias=True)...\n",
      "--> can run with partials\n",
      "Considering cascading modules [Linear(in_features=2024, out_features=10, bias=True)]\n",
      "Pruning 1023 units from Linear(in_features=2024, out_features=10, bias=True) (in)\n",
      "Pruning 1023 units from Linear(in_features=2024, out_features=2024, bias=True) (out)\n",
      "Test set: Average loss: 2.2627, Accuracy: 1835/10000 (18.350%)\n",
      "\n",
      "Computing Shapley values on Linear(in_features=3072, out_features=2024, bias=True)...\n",
      "--> can run with partials\n",
      "Considering cascading modules [Linear(in_features=2024, out_features=2024, bias=True)]\n",
      "Pruning 780 units from Linear(in_features=2024, out_features=2024, bias=True) (in)\n",
      "Pruning 780 units from Linear(in_features=3072, out_features=2024, bias=True) (out)\n",
      "Test set: Average loss: 2.2350, Accuracy: 1989/10000 (19.890%)\n",
      "\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "           Flatten-1                 [-1, 3072]               0\n",
      "            Linear-2                 [-1, 1244]       3,822,812\n",
      "         LeakyReLU-3                 [-1, 1244]               0\n",
      "            Linear-4                 [-1, 1001]       1,246,245\n",
      "         LeakyReLU-5                 [-1, 1001]               0\n",
      "            Linear-6                   [-1, 10]          10,020\n",
      "================================================================\n",
      "Total params: 5,079,077\n",
      "Trainable params: 5,079,077\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.06\n",
      "Params size (MB): 19.38\n",
      "Estimated Total Size (MB): 19.44\n",
      "----------------------------------------------------------------\n",
      "Test set: Average loss: 2.2350, Accuracy: 1989/10000 (19.890%)\n",
      "\n",
      "CPU times: user 23.4 s, sys: 5.06 s, total: 28.4 s\n",
      "Wall time: 33.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Load dataset\n",
    "train_loader, val_loader, test_loader = cifar10.get_dataset_and_loaders(val_split=1000, val_batch_size=1000)\n",
    "loss = cifar10.loss\n",
    "input_size = (3, 32, 32)\n",
    "\n",
    "# Print layer architecture and test performance\n",
    "model, name = cifar10.get_fc_model_with_name()\n",
    "model.to(device)\n",
    "summary(model, input_size=input_size, device=device.type)\n",
    "test(model, device, loss, test_loader);\n",
    "\n",
    "# Define prunable layers\n",
    "layers = list(model.fc.children())\n",
    "prunable_layers = [\n",
    "    # (module_to_prune -> [modules_for_cascading_pruning])\n",
    "    (layers[1], [layers[3]]),\n",
    "    (layers[3], [layers[5]]),\n",
    "]\n",
    "\n",
    "pruner = Pruner(model, input_size, device)\n",
    "attribution = ShapleyAttributionMetric(model, val_loader, loss, device, sv_samples=5)\n",
    "    \n",
    "# Prune layers starting from the outermost\n",
    "for module, cascading_modules in prunable_layers[::-1]:\n",
    "    # Compute Shapley Value attributions\n",
    "    attr, _ = attribution.run([module])[0]\n",
    "    # Select indices corresponding to negative attributions\n",
    "    pruning_indices = np.argwhere(attr < 0).flatten()\n",
    "    # Perform pruning\n",
    "    pruner.prune_model(module, pruning_indices, cascading_modules=cascading_modules)\n",
    "    test(model, device, loss, test_loader);\n",
    "\n",
    "# Test final model\n",
    "summary(model, input_size=input_size, device=device.type)\n",
    "test(model, device, loss, test_loader);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
